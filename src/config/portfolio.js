// ========================================
// CONFIGURA√á√ÉO DO PORTF√ìLIO DATA & ANALYTICS
// ========================================
// Edite este arquivo para personalizar seu portf√≥lio

/**
 * Informa√ß√µes Pessoais do Profissional
 * @description Dados b√°sicos e contatos do portf√≥lio
 */
export const personalInfo = {
  name: "Tiago da Silva e Santo",
  fullName: "Tiago Da Silva E Santo",
  title: "Eng. de Dados | Data Science",
  subtitle: "Transformando dados em insights estrat√©gicos para neg√≥cios",
  location: "S√£o Paulo, SP",
  email: "tiagomars233@gmail.com",
  phone: "+55 11 97542-9994",
  linkedin: "https://www.linkedin.com/in/tiagodados",
  github: "https://github.com/tmarsbr",
  whatsapp: "https://wa.me/5511975429994",
  resume: `${process.env.PUBLIC_URL}/certificates/Curriculo_Tiago_Santo_Formatado.pdf`,
  
  // Resumo para a p√°gina inicial (1-2 frases)
  homeIntro: "Desenho e opero pipelines ETL/ELT (Airflow, dbt, Spark), modelo dados para analytics e sirvo features para ML. Foco em escalabilidade, custo e qualidade com IaC, CI/CD e DataOps na AWS.",
  
  // Descri√ß√£o expandida para a se√ß√£o Sobre
  aboutDescription: `Da Oficina para os Dados
Tudo come√ßou com barulho de torno, cheiro de √≥leo e precis√£o milim√©trica. Na usinagem, aprendi que um erro de um d√©cimo pode custar caro. Foi l√° que desenvolvi o olhar cl√≠nico, a paci√™ncia e o gosto por resolver problemas com m√©todo.

Hoje, essa precis√£o virou c√≥digo. Projeto pipelines, organizo dados brutos, crio dashboards, desenvolvo modelos preditivos e conto hist√≥rias com dados reais. Da engenharia √† an√°lise, passando pela ci√™ncia de dados, aplico o mesmo rigor t√©cnico que aprendi na mec√¢nica para transformar n√∫meros em decis√µes inteligentes.

Toler√¢ncia apertada e processo confi√°vel geram qualidade ‚Äî em a√ßo e em dados.`,


  // Avatar/Foto (deixe vazio para usar placeholder)
  avatar: `${process.env.PUBLIC_URL}/imagem/perfil.jpeg`, // Adicione o caminho da sua foto aqui
};

/**
 * Categorias de Projetos
 * @description Lista de categorias principais para filtros
 */
export const PROJECT_CATEGORIES = [
  'Todos',
  'An√°lise de Dados',
  'Engenharia de Dados',
  'Ci√™ncia de Dados',
  'API & Scraping'
];

/**
 * Subcategorias por Categoria
 * @description Subfiltros espec√≠ficos para cada categoria principal
 */
export const PROJECT_SUBCATEGORIES = {
  'Engenharia de Dados': ['IaC', 'CI/CD', 'ETL/ELT', 'Cloud AWS', 'DataOps'],
  'An√°lise de Dados': ['EDA', 'Visualiza√ß√£o', 'Estat√≠stica', 'Experimenta√ß√£o'],
  'Ci√™ncia de Dados': ['ML Cl√°ssico', 'Deep Learning', 'MLOps', 'NLP'],
  'API & Scraping': ['Scraping', 'API REST', 'Integra√ß√µes'],
  'Todos': []
};

/**
 * √Årvore de Skills - Tecnologias por Subcategoria
 * @description Mapeamento de tecnologias espec√≠ficas para cada subcategoria
 * @structure { categoria: { subcategoria: [tecnologias] } }
 */
export const SKILL_TREE = {
  'Engenharia de Dados': {
    'IaC': ['Terraform'],
    'CI/CD': ['GitHub Actions', 'Docker', 'Kubernetes'],
    'ETL/ELT': ['Airbyte', 'dbt', 'Apache Spark', 'Python'],
    'Cloud AWS': ['S3', 'EMR', 'Redshift', 'Glue', 'Athena', 'DynamoDB', 'QuickSight', 'SageMaker'],
    'DataOps': ['Apache Airflow']
  },
  'Data Science': {
    'Data Science': ['EDA', 'Visualiza√ß√£o', 'Feature Engineering', 'Storytelling', 'Causalidade', 'S√©ries Temporais'],
    'Python': ['pandas', 'numpy', 'matplotlib', 'seaborn', 'plotly', 'scipy', 'polars', 'requests', 'jupyter'],
    'Machine Learning': ['scikit-learn', 'XGBoost', 'LightGBM', 'CatBoost', 'statsmodels', 'Optuna', 'OneHotEncoder', 'StandardScaler', 'Pipeline', 'Cross-Validation'],
    'SQL': ['PostgreSQL', 'MySQL', 'SQLite', 'SQL Server', 'Oracle', 'Redshift', 'Athena', 'DuckDB'],
    'Estat√≠stica': ['Descritiva', 'Infer√™ncia', 'Testes de Hip√≥tese', 'Regress√£o', 'ANOVA', 'P-valor', 'Intervalo de Confian√ßa', 'Correla√ß√£o', 'S√©ries Temporais']
  }
};

/**
 * Habilidades T√©cnicas Organizadas por Categoria
 * @description Stack tecnol√≥gico com n√≠veis de profici√™ncia (0-100)
 * @structure { categoria: [{ name, level, icon }] }
 */
export const skills = {
  "Linguagens & Bancos de Dados": [
    { name: "Python", level: 90, icon: "üêç" },
    { name: "SQL", level: 85, icon: "üóÉÔ∏è" },
    { name: "Web Scraping", level: 80, icon: "üï∑Ô∏è" },
    { name: "SQLite", level: 85, icon: "üíæ" },
    { name: "PostgreSQL", level: 80, icon: "üêò" },
    { name: "MySQL", level: 80, icon: "üóÑÔ∏è" },
    { name: "Oracle", level: 70, icon: "üìä" },
    { name: "MongoDB", level: 75, icon: "üçÉ" },
  ],
  "Machine Learning & Estat√≠stica": [
    { name: "Scikit-learn", level: 85, icon: "ü§ñ" },
    { name: "Pandas", level: 90, icon: "üêº" },
    { name: "NumPy", level: 85, icon: "üî¢" },
    { name: "SciPy", level: 80, icon: "üìà" },
    { name: "Regress√£o", level: 85, icon: "üìâ" },
    { name: "Classifica√ß√£o", level: 85, icon: "üéØ" },
    { name: "Clusteriza√ß√£o", level: 80, icon: "üìä" },
    { name: "Estat√≠stica Descritiva", level: 90, icon: "üìã" },
  ],
  "Visualiza√ß√£o & BI": [
    { name: "Matplotlib", level: 85, icon: "üìä" },
    { name: "Seaborn", level: 85, icon: "üé®" },
    { name: "Plotly", level: 80, icon: "üìà" },
    { name: "Power BI", level: 75, icon: "‚ö°" },
  ],
  "Desenvolvimento & Cloud": [
    { name: "Git/GitHub", level: 85, icon: "üîÑ" },
    { name: "Docker", level: 70, icon: "üê≥" },
    { name: "Streamlit", level: 80, icon: "üöÄ" },
    { name: "APIs Python", level: 75, icon: "üîå" },
    { name: "AWS", level: 70, icon: "‚òÅÔ∏è" },
  ],
};

/**
 * Experi√™ncias Profissionais e Educacionais
 * @description Timeline da carreira profissional e forma√ß√£o acad√™mica
 * @structure Array de objetos com: id, type, title, company, period, description, skills, icon
 */
export const experiences = [
  {
    id: 1,
    type: "work",
    title: "Auxiliar de Produ√ß√£o",
    company: "LJ Correia Equipamentos LTDA",
    period: "01/02/2018 - 07/05/2021",
    description: "Apoio √† linha de produ√ß√£o industrial com foco em organiza√ß√£o, controle de qualidade e efici√™ncia operacional.",
    skills: ["Produ√ß√£o Industrial", "Controle de Qualidade", "Organiza√ß√£o"],
    icon: "‚ö°"
  },
  {
    id: 2,
    type: "work",
    title: "Torneiro Mec√¢nico",
    company: "Cilintec Cilindros para Impress√µes LTDA",
    period: "15/07/2022 - 02/09/2023",
    description: "Usinagem de pe√ßas met√°licas de precis√£o utilizando tornos CNC e convencionais. Foco em qualidade, atendimento t√©cnico e entrega conforme especifica√ß√µes rigorosas.",
    skills: ["Torno CNC", "Leitura de Desenho T√©cnico", "Controle de Qualidade"],
    icon: "üîß"
  },
  {
    id: 3,
    type: "work",
    title: "Torneiro Mec√¢nico Aut√¥nomo",
    company: "Trabalho Aut√¥nomo",
    period: "09/2023 - Presente",
    description: "Especialista em usinagem de cadinhos de grafite para fundi√ß√£o, atendendo demandas personalizadas de clientes. Respons√°vel por todo o processo, desde o planejamento at√© a entrega final.",
    skills: ["Usinagem de Precis√£o", "Gest√£o de Projetos", "Atendimento ao Cliente"],
    icon: "‚öôÔ∏è"
  },
  {
    id: 4,
    type: "education",
    title: "Transi√ß√£o para Data & Analytics",
    company: "Pod Academy",
    period: "1/12/2023 - 8/12/2025",
    description: "A Forma√ß√£o Full Stack Dados & Analytics √© a trilha completa de Dados & Analytics, com Engenharia de Dados, Tecnologia, Computa√ß√£o em Nuvem, Neg√≥cios e Ci√™ncia de Dados do n√≠vel b√°sico at√© o especialista.",
    skills: ["Engenharia de Dados", "Computa√ß√£o em Nuvem", "Ci√™ncia de Dados", "Neg√≥cios"],
    icon: "üìä"
  }
];

/**
 * Portf√≥lio de Projetos em Data & Analytics
 * @description Projetos pr√°ticos demonstrando habilidades t√©cnicas
 * @categories "An√°lise Explorat√≥ria", "Engenharia de Dados", "Ci√™ncia de Dados", "API & Web Scraping"
 * @structure Array de objetos com dados completos do projeto
 */
export const projects = [
  {
    id: 1,
    title: "An√°lise Explorat√≥ria - Spotify Most Streamed Songs",
    impactPhrase: "‚òÖ Destaque | An√°lise de Dados",
    description: "üéµ Transformei +50k m√∫sicas em insights visuais que revelam padr√µes de sucesso no Spotify usando Python e visualiza√ß√µes com Seaborn.",
    longDescription: "Mergulhei no universo musical para responder uma pergunta intrigante: o que torna uma m√∫sica irresist√≠vel? Usando dataset do Spotify com as faixas mais tocadas globalmente, conduzi uma an√°lise explorat√≥ria completa que revelou insights surpreendentes. Descobri que caracter√≠sticas como 'danceability' e 'energy' t√™m correla√ß√µes espec√≠ficas com o sucesso, mas tamb√©m identifiquei padr√µes temporais que mostram como o gosto musical evolui. O projeto culminou na cria√ß√£o de um 'mapa do sucesso musical' com 8 fatores-chave que podem prever a popularidade de uma m√∫sica.",
    technologies: ["Python", "Pandas", "Matplotlib", "Seaborn", "Jupyter"],
    category: "An√°lise Explorat√≥ria",
    subcategories: ["EDA", "Visualiza√ß√£o", "Estat√≠stica"],
    image: `${process.env.PUBLIC_URL}/projects/capa_spotify_analysis.png`,
    github: "https://github.com/tmarsbr/data-analyst-project",
    demo: "",
    metrics: "An√°lise de +50k m√∫sicas, identifica√ß√£o de 8 fatores-chave de sucesso",
    featured: true,
    complexity: 4,
    date: "2024"
  },
  {
    id: 2,
    title: "An√°lise dos Acidentes nas Rodovias Brasileiras",
    impactPhrase: "üéØ Projeto Social | Geoan√°lise",
    description: "üõ£Ô∏è Analisei +100k registros de acidentes da PRF criando um mapa inteligente de seguran√ßa vi√°ria que identifica pontos cr√≠ticos em 27 estados brasileiros.",
    longDescription: "Este projeto nasceu de uma miss√£o pessoal: usar dados para salvar vidas nas estradas. Analisando registros da Pol√≠cia Rodovi√°ria Federal, criei visualiza√ß√µes interativas que revelam os pontos cr√≠ticos de acidentes em todo territ√≥rio nacional. O mais impactante foi descobrir padr√µes inesperados entre localiza√ß√£o de radares e redu√ß√£o de acidentes, gerando insights que podem influenciar pol√≠ticas p√∫blicas de seguran√ßa. Mapiei 27 estados e identifiquei os hor√°rios, condi√ß√µes clim√°ticas e trechos mais perigosos, criando um verdadeiro 'GPS da seguran√ßa' para as rodovias brasileiras.",
    technologies: ["Python", "Pandas", "Geopandas", "Plotly", "Folium"],
    category: "An√°lise Explorat√≥ria",
    subcategories: ["EDA", "Visualiza√ß√£o", "Estat√≠stica"],
    image: `${process.env.PUBLIC_URL}/projects/capa_prf_accidents.png`,
    github: "https://github.com/tmarsbr/analise-PRF-",
    demo: "",
    metrics: "An√°lise de +100k acidentes, mapeamento de 27 estados",
    featured: true,
    complexity: 5,
    date: "2024"
  },
  {
    id: 3,
    title: "Pipeline de Integra√ß√£o - Cl√≠nicas Sanare e Reviver",
    impactPhrase: "‚ö° Projeto Real | Engenharia de Dados",
    description: "üè• Desenvolvi um pipeline ETL robusto que unificou sistemas de duas cl√≠nicas m√©dicas, migrando +10k registros com 99.9% de precis√£o e zero downtime.",
    longDescription: "Enfrentei um desafio real do mundo corporativo: duas cl√≠nicas m√©dicas se fundiram e precisavam unificar seus dados de pacientes, hist√≥ricos e procedimentos. O problema? Sistemas completamente diferentes, formatos incompat√≠veis e zero margem para erros - afinal, eram dados de sa√∫de humana. Desenvolvi uma solu√ß√£o elegante usando programa√ß√£o orientada a objetos, criando um pipeline ETL modular que n√£o apenas integrou os dados, mas tamb√©m implementou valida√ß√µes rigorosas de qualidade. O resultado? Uma migra√ß√£o 100% bem-sucedida que permitiu √† nova empresa operar desde o primeiro dia.",
    technologies: ["Python", "OOP", "ETL", "Data Quality", "Pandas"],
    category: "Engenharia de Dados",
    subcategories: ["ETL/ELT"],
    image: `${process.env.PUBLIC_URL}/projects/capa_integracao_sistemas_medicos.png`,
    github: "https://github.com/tmarsbr/projeto_pipeline",
    demo: "",
    metrics: "Integra√ß√£o de +10k registros, 99.9% de precis√£o na migra√ß√£o",
    featured: true,
    complexity: 5,
    date: "2024"
  },
  {
    id: 4,
    title: "Extra√ß√£o e An√°lise - Reposit√≥rios GitHub",
    impactPhrase: "üî• Automatiza√ß√£o | Engenharia de Dados",
    description: "üêô Sistema automatizado que extraiu e analisou dados de +1000 reposit√≥rios de 15 grandes empresas tech via API GitHub, revelando tend√™ncias de desenvolvimento.",
    longDescription: "Sistema automatizado para coleta e an√°lise de dados de reposit√≥rios GitHub de grandes empresas de tecnologia. Utilizou API do GitHub para extrair informa√ß√µes sobre linguagens, atividade e tend√™ncias de desenvolvimento, gerando insights sobre o ecossistema tech.",
    technologies: ["Python", "GitHub API", "Pandas", "Requests", "JSON"],
    category: "Engenharia de Dados",
    subcategories: ["ETL/ELT"],
    image: `${process.env.PUBLIC_URL}/projects/capa_github_analysis.png`,
    github: "https://github.com/tmarsbr/Projeto_api",
    demo: "",
    metrics: "An√°lise de +1000 reposit√≥rios, 15 empresas tech",
    featured: false,
    complexity: 3,
    date: "2024"
  },
  {
    id: 5,
    title: "Pipeline Python - MongoDB - MySQL",
    impactPhrase: "üöÄ Integra√ß√£o | Engenharia de Dados",
    description: "üîÑ Pipeline completo para e-commerce integrando MongoDB e MySQL, reduzindo em 70% o tempo de an√°lise da equipe de BI com processamento automatizado.",
    longDescription: "Desenvolvimento de pipeline completo para processamento de dados de e-commerce, integrando diferentes bases de dados. Solu√ß√£o automatizada para ETL entre MongoDB (dados n√£o-estruturados) e MySQL (dados estruturados), com foco em performance e confiabilidade.",
    technologies: ["Python", "MongoDB", "MySQL", "ETL", "PyMongo"],
    category: "Engenharia de Dados",
    subcategories: ["ETL/ELT"],
    image: `${process.env.PUBLIC_URL}/projects/capa_pipeline_mongo_mysql.png`,
    github: "https://github.com/tmarsbr/pipeline-python-mongo-mysql",
    demo: "",
    metrics: "Redu√ß√£o de 70% no tempo de an√°lise da equipe de BI",
    featured: true,
    complexity: 4,
    date: "2024"
  },
  {
    id: 6,
    title: "An√°lise de Cr√©dito com Machine Learning",
    impactPhrase: "üõ†Ô∏è Em Desenvolvimento | Ci√™ncia de Dados",
    description: "üí≥ Modelo de score de cr√©dito com Machine Learning em fase de testes.",
    longDescription: "Projeto de an√°lise de cr√©dito utilizando t√©cnicas de machine learning para avalia√ß√£o de risco. Em desenvolvimento com foco em algoritmos de classifica√ß√£o e an√°lise de padr√µes de inadimpl√™ncia.",
    technologies: ["Python", "Scikit-learn", "Pandas", "Machine Learning"],
    category: "Ci√™ncia de Dados",
    subcategories: ["ML Cl√°ssico"],
    image: `${process.env.PUBLIC_URL}/projects/capa_credito_ml.png`,
    github: "",
    demo: "",
    metrics: "",
    featured: false,
    inDevelopment: true,
    date: "Em breve"
  },
  {
    id: 7,
    title: "People Analytics - Insights de RH",
    impactPhrase: "üöß Em Desenvolvimento | Ci√™ncia de Dados",
    description: "üë• Sistema de an√°lise de dados de RH para insights estrat√©gicos em gest√£o de pessoas.",
    longDescription: "Projeto focado na aplica√ß√£o de People Analytics para tomada de decis√£o em gest√£o de pessoas, incluindo an√°lise de turnover, performance e engajamento de colaboradores.",
    technologies: ["Python", "Pandas", "Plotly", "Statistics"],
    category: "Ci√™ncia de Dados",
    subcategories: ["ML Cl√°ssico"],
    image: `${process.env.PUBLIC_URL}/projects/capa_people_analytics.png`,
    github: "",
    demo: "",
    metrics: "",
    featured: false,
    inDevelopment: true,
    date: "Em breve"
  },
  {
    id: 8,
    title: "Previs√£o de Demandas - S√©ries Temporais",
    impactPhrase: "‚è±Ô∏è Em Constru√ß√£o | Ci√™ncia de Dados",
    description: "üìà Modelos de previs√£o de demanda utilizando algoritmos de s√©ries temporais.",
    longDescription: "Projeto focado em previs√£o de demandas utilizando algoritmos de s√©rie temporal avan√ßados, incluindo ARIMA, Prophet e redes neurais para forecasting empresarial.",
    technologies: ["Python", "Prophet", "ARIMA", "TensorFlow"],
    category: "Ci√™ncia de Dados",
    subcategories: ["ML Cl√°ssico"],
    image: `${process.env.PUBLIC_URL}/projects/capa_previsao_demandas.png`,
    github: "",
    demo: "",
    metrics: "",
    featured: false,
    inDevelopment: true,
    date: "Em breve"
  },
  {
    id: 9,
    title: "Sistema Antifraude com IA",
    impactPhrase: "üîí Em Desenvolvimento | Ci√™ncia de Dados",
    description: "üõ°Ô∏è Sistema de detec√ß√£o de fraudes com m√©todos estat√≠sticos e machine learning.",
    longDescription: "Modelo de escore antifraude utilizando t√©cnicas avan√ßadas de machine learning para detectar padr√µes suspeitos e prevenir fraudes em transa√ß√µes financeiras.",
    technologies: ["Python", "Scikit-learn", "Anomaly Detection", "Deep Learning"],
    category: "Ci√™ncia de Dados",
    subcategories: ["ML Cl√°ssico"],
    image: `${process.env.PUBLIC_URL}/projects/capa_fraude_financeira.png`,
    github: "",
    demo: "",
    metrics: "",
    featured: false,
    inDevelopment: true,
    date: "Em breve"
  },
  {
    id: 10,
    title: "Automatizando Infraestrutura de Processamento de Dados com AWS EMR e Apache Flink",
    impactPhrase: "‚òÅÔ∏è Cloud Infrastructure | Engenharia de Dados",
    description: "‚ö° Infraestrutura como c√≥digo para processamento de big data em tempo real utilizando AWS EMR, Apache Flink e Terraform para escalabilidade autom√°tica.",
    longDescription: "Projeto focado na automa√ß√£o completa de infraestrutura de processamento de dados em nuvem. Utilizando AWS EMR (Elastic MapReduce) para clusters gerenciados e Apache Flink para processamento de streams em tempo real, toda a infraestrutura √© provisionada via Terraform seguindo pr√°ticas de IaC (Infrastructure as Code). O sistema inclui auto-scaling, monitoramento integrado e otimiza√ß√£o de custos, demonstrando como construir pipelines de dados robustos e escal√°veis na AWS.",
    technologies: ["AWS EMR", "Apache Flink", "Terraform", "IaC", "Python"],
    category: "Engenharia de Dados",
    subcategories: ["Cloud AWS", "IaC"],
    image: `${process.env.PUBLIC_URL}/projects/capa_aws_emr_flink.png`,
    github: "https://github.com/tmarsbr/aws-emr-flink-terraform",
    demo: "",
    metrics: "Infraestrutura 100% automatizada, processamento em tempo real",
    featured: true,
    complexity: 5,
    date: "2024"
  },
  {
    id: 11,
    title: "Pipeline PySpark Para Extrair, Transformar e Carregar Arquivos JSON em Banco de Dados",
    impactPhrase: "üî• Big Data Processing | Engenharia de Dados",
    description: "üöÄ Pipeline robusto de ETL desenvolvido com PySpark para processar grandes volumes de dados JSON, aplicando transforma√ß√µes complexas e carregamento otimizado.",
    longDescription: "Solu√ß√£o completa de ETL (Extract, Transform, Load) utilizando Apache Spark via PySpark para processar arquivos JSON de grande volume. O pipeline implementa transforma√ß√µes complexas incluindo limpeza de dados, normaliza√ß√£o, agrega√ß√µes e valida√ß√µes de qualidade. Otimizado para performance com t√©cnicas de particionamento, cache inteligente e processamento distribu√≠do, garantindo escalabilidade para datasets de qualquer tamanho.",
    technologies: ["PySpark", "Apache Spark", "JSON", "ETL", "SQL"],
    category: "Engenharia de Dados",
    subcategories: ["ETL/ELT"],
    image: `${process.env.PUBLIC_URL}/projects/capa_pyspark_etl_json.png`,
    github: "https://github.com/tmarsbr/pyspark-json-etl-pipeline",
    demo: "",
    metrics: "Processamento distribu√≠do, transforma√ß√µes complexas",
    featured: true,
    complexity: 4,
    date: "2024"
  },
  {
    id: 12,
    title: "Pipeline de Limpeza e Transforma√ß√£o Para Aplica√ß√µes de IA com PySpark SQL",
    impactPhrase: "ü§ñ AI Data Preparation | Engenharia de Dados",
    description: "‚ú® Pipeline especializado em prepara√ß√£o de dados para modelos de IA usando PySpark SQL, garantindo qualidade e consist√™ncia dos datasets de treinamento.",
    longDescription: "Pipeline avan√ßado de prepara√ß√£o de dados especificamente desenhado para alimentar aplica√ß√µes de Intelig√™ncia Artificial. Utilizando PySpark SQL para opera√ß√µes eficientes, o sistema implementa t√©cnicas sofisticadas de limpeza, detec√ß√£o de anomalias, feature engineering e normaliza√ß√£o. Inclui valida√ß√µes automatizadas de qualidade, tratamento inteligente de valores ausentes e gera√ß√£o de estat√≠sticas descritivas para garantir que os dados estejam prontos para treinamento de modelos de ML.",
    technologies: ["PySpark", "Spark SQL", "Feature Engineering", "Data Quality"],
    category: "Engenharia de Dados",
    subcategories: ["ETL/ELT"],
    image: `${process.env.PUBLIC_URL}/projects/capa_pyspark_ai_pipeline.png`,
    github: "https://github.com/tmarsbr/pyspark-ai-data-pipeline",
    demo: "",
    metrics: "Prepara√ß√£o de dados para IA, valida√ß√µes automatizadas",
    featured: true,
    complexity: 5,
    date: "2024"
  },
  {
    id: 13,
    title: "Automa√ß√£o de Testes de Modelos de Dados no dbt",
    impactPhrase: "üß™ Data Testing | Engenharia de Dados",
    description: "üîç Framework completo de testes automatizados para modelos de dados usando dbt, garantindo qualidade e confiabilidade dos pipelines anal√≠ticos.",
    longDescription: "Implementa√ß√£o de um framework robusto de testes automatizados para modelos de dados utilizando dbt (data build tool). O sistema inclui testes de integridade referencial, valida√ß√µes de qualidade de dados, testes de regress√£o e monitoramento cont√≠nuo. Desenvolvido com foco em DataOps, o projeto demonstra como implementar CI/CD para dados, incluindo testes unit√°rios para transforma√ß√µes SQL, valida√ß√µes de schema e alertas autom√°ticos para anomalias nos dados.",
    technologies: ["dbt", "SQL", "Data Testing", "DataOps", "CI/CD"],
    category: "Engenharia de Dados",
    subcategories: ["DataOps"],
    image: `${process.env.PUBLIC_URL}/projects/capa_data_warehouse_local.png`,
    github: "https://github.com/tmarsbr/dbt-automated-testing",
    demo: "",
    metrics: "Framework de testes automatizados, qualidade de dados garantida",
    featured: true,
    complexity: 4,
    date: "2024"
  },
  {
    id: 14,
    title: "Movimenta√ß√£o de Dados Entre Bancos de Dados com Airbyte",
    impactPhrase: "üîÑ Data Integration | Engenharia de Dados",
    description: "üåê Solu√ß√£o de integra√ß√£o de dados usando Airbyte para sincroniza√ß√£o autom√°tica entre diferentes fontes de dados, garantindo consist√™ncia e atualiza√ß√£o em tempo real.",
    longDescription: "Implementa√ß√£o de uma solu√ß√£o completa de integra√ß√£o de dados utilizando Airbyte para orquestrar a movimenta√ß√£o entre diferentes sistemas de banco de dados. O projeto demonstra como configurar conectores personalizados, implementar transforma√ß√µes durante a sincroniza√ß√£o e garantir a consist√™ncia dos dados entre ambientes. Inclui monitoramento de performance, tratamento de falhas e estrat√©gias de recupera√ß√£o, mostrando como construir pipelines de dados resilientes e escal√°veis.",
    technologies: ["Airbyte", "PostgreSQL", "MySQL", "Data Integration", "ETL"],
    category: "Engenharia de Dados",
    subcategories: ["ETL/ELT"],
    image: `${process.env.PUBLIC_URL}/projects/capa_automacao_etl.png`,
    github: "https://github.com/tmarsbr/airbyte-data-movement",
    demo: "",
    metrics: "Sincroniza√ß√£o autom√°tica entre DBs, integra√ß√£o de dados resiliente",
    featured: true,
    complexity: 4,
    date: "2024"
  }
];

/**
 * Certifica√ß√µes e Forma√ß√µes Acad√™micas
 * @description Certificados obtidos em cursos e forma√ß√µes
 * @structure Array com dados dos certificados e links para PDFs
 */
export const certificates = [
  {
    id: 1,
    title: "Nivelamento em Matem√°tica, Estat√≠stica e Programa√ß√£o para Big Data & Analytics",
    institution: "PoD Academy",
    year: "2024",
    duration: "120h",
    image: `${process.env.PUBLIC_URL}/certificates/1.png`,
    pdf: `${process.env.PUBLIC_URL}/certificates/certificado-2026662A6157507713239070.pdf`,
    skills: ["Matem√°tica", "Estat√≠stica", "Programa√ß√£o", "Big Data"]
  },
  {
    id: 2,
    title: "Ci√™ncias Aplicadas: Matem√°tica e Estat√≠stica",
    institution: "PoD Academy",
    year: "2025",
    duration: "80h",
    image: `${process.env.PUBLIC_URL}/certificates/4.png`,
    pdf: `${process.env.PUBLIC_URL}/certificates/certificado-2427912AD2AF41E914203880.pdf`,
    skills: ["Matem√°tica Aplicada", "Estat√≠stica Aplicada", "Dados"]
  },
  {
    id: 3,
    title: "Forma√ß√£o em An√°lise de Dados",
    institution: "PoD Academy",
    year: "2024",
    duration: "100h",
    image: `${process.env.PUBLIC_URL}/certificates/3.png`,
    pdf: `${process.env.PUBLIC_URL}/certificates/certificado-2184196A1D520FC214203880.pdf`,
    skills: ["An√°lise de Dados", "CRISP-DM", "Estat√≠stica", "Visualiza√ß√£o"]
  },
  {
    id: 4,
    title: "Cloud Computing",
    institution: "PoD Academy",
    year: "2024",
    duration: "60h",
    image: `${process.env.PUBLIC_URL}/certificates/2.png`,
    pdf: `${process.env.PUBLIC_URL}/certificates/certificado-2181848AF6E35A5F13239070.pdf`,
    skills: ["Cloud Computing", "AWS", "Azure", "GCP"]
  },
  {
    id: 5,
    title: "An√°lise de Neg√≥cios",
    institution: "Forma√ß√£o em Ci√™ncia de Dados",
    year: "2024",
    duration: "40h",
    image: `${process.env.PUBLIC_URL}/certificates/capa_certificado_analise_negocios.png`,
    pdf: "",
    skills: ["An√°lise de Neg√≥cios", "KPIs", "M√©tricas", "Estrat√©gia"]
  },
  {
    id: 6,
    title: "Regress√£o Linear - Python",
    institution: "Data Viking",
    year: "2024",
    duration: "20h",
    image: `${process.env.PUBLIC_URL}/certificates/capa_certificado_regressao_linear.png`,
    pdf: "",
    skills: ["Python", "Regress√£o Linear", "Machine Learning", "Estat√≠stica"]
  },
  {
    id: 7,
    title: "Floresta Aleat√≥ria (Random Forest)",
    institution: "Data Viking",
    year: "2024",
    duration: "25h",
    image: `${process.env.PUBLIC_URL}/certificates/capa_certificado_random_forest.png`,
    pdf: "",
    skills: ["Random Forest", "Machine Learning", "Python", "Algoritmos"]
  },
  {
    id: 8,
    title: "Regress√£o Log√≠stica - Python",
    institution: "Data Viking",
    year: "2024",
    duration: "22h",
    image: `${process.env.PUBLIC_URL}/certificates/capa_certificado_regressao_logistica.png`,
    pdf: "",
    skills: ["Regress√£o Log√≠stica", "Python", "Machine Learning", "Classifica√ß√£o"]
  },
  {
    id: 9,
    title: "SQL com Python",
    institution: "Data Viking",
    year: "2024",
    duration: "30h",
    image: `${process.env.PUBLIC_URL}/certificates/capa_certificado_sql_python.png`,
    pdf: "",
    skills: ["SQL", "Python", "Banco de Dados", "SQLite"]
  },
  {
    id: 10,
    title: "Forma√ß√£o Python Impressionador",
    institution: "Hashtag Treinamentos",
    year: "2024",
    duration: "50h",
    image: `${process.env.PUBLIC_URL}/certificates/capa_certificado_python_impressionador.png`,
    pdf: "",
    skills: ["Python", "Automa√ß√£o", "Data Science", "Pandas"]
  },
  {
    id: 11,
    title: "Forma√ß√£o SQL Impressionador",
    institution: "Hashtag Treinamentos",
    year: "2024",
    duration: "45h",
    image: `${process.env.PUBLIC_URL}/certificates/capa_certificado_sql_impressionador.png`,
    pdf: "",
    skills: ["SQL", "Banco de Dados", "Consultas Avan√ßadas", "Performance"]
  },
  {
    id: 12,
    title: "Git e GitHub Essencial",
    institution: "DIO",
    year: "2023",
    duration: "15h",
    image: "",
    pdf: "",
    skills: ["Git", "GitHub", "Controle de Vers√£o", "Colabora√ß√£o"]
  }
];

/**
 * Configura√ß√µes de Tema e Estilo
 * @description Paleta de cores, tipografia e espa√ßamentos para modo claro/escuro
 * @includes cores, fontes, espa√ßamentos, bordas e transi√ß√µes
 */
export const themeConfig = {
  // Cores principais - refinadas para melhor contraste e harmonia
  light: {
    primaryColor: "#1565c0", // Azul mais vibrante e profissional
    secondaryColor: "#d32f2f", // Vermelho mais equilibrado
    accentColor: "#00897b", // Verde-azulado sofisticado
    backgroundColor: "#fafafa",
    paperColor: "#ffffff",
    alternateBackground: "#f5f7fa", // Off-white para se√ß√µes alternadas
    textPrimary: "#2c3e50", // Texto mais escuro para melhor contraste
    textSecondary: "#64748b", // Cinza moderno
    dividerColor: "#e2e8f0",
    shadow: "0 4px 20px rgba(0, 0, 0, 0.08)",
    hoverShadow: "0 12px 40px rgba(0, 0, 0, 0.15)"
  },
  dark: {
    primaryColor: "#64b5f6", // Azul suave para fundo escuro
    secondaryColor: "#ef5350", // Vermelho suave
    accentColor: "#4db6ac", // Verde-azulado suave
    backgroundColor: "#0f172a", // Azul muito escuro
    paperColor: "#1e293b", // Cinza-azul escuro para cards
    alternateBackground: "#1a202c", // Cinza escuro para se√ß√µes alternadas
    textPrimary: "#f1f5f9", // Branco quase puro
    textSecondary: "#94a3b8", // Cinza claro
    dividerColor: "#334155",
    shadow: "0 4px 20px rgba(0, 0, 0, 0.3)",
    hoverShadow: "0 12px 40px rgba(0, 0, 0, 0.5)"
  },
  
  // Tipografia aprimorada com Google Fonts
  fontFamily: "'Inter', 'Roboto', 'Segoe UI', 'Helvetica Neue', sans-serif",
  fontSize: {
    xs: "0.75rem",
    small: "0.875rem", 
    medium: "1rem",
    large: "1.25rem",
    xlarge: "2rem",
    xxlarge: "3rem",
    xxxlarge: "3.75rem"
  },
  fontWeight: {
    light: 300,
    regular: 400,
    medium: 500,
    semibold: 600,
    bold: 700
  },
  
  // Espa√ßamentos seguindo o padr√£o 8px
  spacing: {
    xs: "4px",
    sm: "8px",
    md: "16px",
    lg: "24px",
    xl: "32px",
    xxl: "48px",
    xxxl: "64px"
  },
  
  // Border radius padronizado
  borderRadius: {
    small: "6px",
    medium: "12px",
    large: "16px",
    xl: "24px",
    round: "50%"
  },
  
  // Anima√ß√µes e transi√ß√µes
  transitions: {
    fast: "0.15s ease-out",
    normal: "0.3s ease-in-out", 
    slow: "0.5s ease-in-out"
  }
};

/**
 * Configura√ß√µes de SEO (Search Engine Optimization)
 * @description Metadados para otimiza√ß√£o em motores de busca
 * @includes title, description, keywords, Open Graph, Twitter Cards
 */
export const seoConfig = {
  title: "Tiago Silva ‚Äî Eng. de Dados | Data Science",
  description: "Engenharia de Dados e Data Science: ETL/ELT (Airflow, dbt, Spark), analytics e features para ML. IaC, CI/CD e DataOps na AWS.",
  keywords: "engenharia de dados, data engineering, data science, analytics, python, machine learning, sql, aws, airflow, dbt, spark, tiago silva, portf√≥lio",
  author: "Tiago Silva",
  url: "https://tmarsbr.github.io/portifolio-data-analytics", // URL do GitHub Pages
  image: `${process.env.PUBLIC_URL}/og-image.jpg`, // Adicione uma imagem de preview
  
  // Open Graph
  ogTitle: "Tiago Silva ‚Äî Eng. de Dados | Data Science",
  ogDescription: "ETL/ELT (Airflow, dbt, Spark), modelagem para analytics e features de ML. IaC, CI/CD e DataOps na AWS.",
  twitterCard: "summary_large_image",
  twitterCreator: "@tiagodados" // Se tiver Twitter
};

/**
 * Configura√ß√µes do Google Analytics
 * @description Tracking e m√©tricas de acesso (opcional)
 */
export const analyticsConfig = {
  trackingId: process.env.REACT_APP_TRACKING_ID || "",
  enabled: process.env.NODE_ENV === "production"
};

/**
 * Configura√ß√£o Principal do Portf√≥lio
 * @description Exporta√ß√£o unificada de todas as configura√ß√µes
 * @exports Objeto contendo todas as se√ß√µes do portf√≥lio
 */
const portfolioConfig = {
  personalInfo,
  skills,
  experiences,
  projects,
  certificates,
  themeConfig,
  seoConfig,
  analyticsConfig
};

export default portfolioConfig;
