/**
 * Categorias de Projetos
 * @description Lista de categorias principais para filtros
 */
export const PROJECT_CATEGORIES = [
    'Todos',
    'An√°lise de Dados',
    'Engenharia de Dados',
    'API & Scraping'
];

/**
 * Subcategorias por Categoria
 * @description Subfiltros espec√≠ficos para cada categoria principal
 */
export const PROJECT_SUBCATEGORIES = {
    'Engenharia de Dados': ['ETL/ELT', 'Cloud AWS', 'SQL'],
    'An√°lise de Dados': ['EDA', 'Visualiza√ß√£o', 'Estat√≠stica', 'Experimenta√ß√£o'],
    'API & Scraping': ['Scraping', 'API REST', 'Integra√ß√µes'],
    'Todos': []
};

/**
 * Portf√≥lio de Projetos em Data & Analytics
 * @description Projetos pr√°ticos demonstrando habilidades t√©cnicas
 * @categories "An√°lise Explorat√≥ria", "Engenharia de Dados", "Ci√™ncia de Dados", "API & Web Scraping"
 * @structure Array de objetos com dados completos do projeto
 */
export const projects = [
    {
        id: 1,
        title: "An√°lise Explorat√≥ria - Spotify Most Streamed Songs",
        impactPhrase: "‚òÖ Destaque | An√°lise de Dados",
        description: "üéµ Transformei +50k m√∫sicas em insights visuais que revelam padr√µes de sucesso no Spotify usando Python e visualiza√ß√µes com Seaborn.",
        longDescription: "Mergulhei no universo musical para responder uma pergunta intrigante: o que torna uma m√∫sica irresist√≠vel? Usando dataset do Spotify com as faixas mais tocadas globalmente, conduzi uma an√°lise explorat√≥ria completa que revelou insights surpreendentes. Descobri que caracter√≠sticas como 'danceability' e 'energy' t√™m correla√ß√µes espec√≠ficas com o sucesso, mas tamb√©m identifiquei padr√µes temporais que mostram como o gosto musical evolui. O projeto culminou na cria√ß√£o de um 'mapa do sucesso musical' com 8 fatores-chave que podem prever a popularidade de uma m√∫sica.",
        technologies: ["Python", "Pandas", "Matplotlib", "Seaborn", "Jupyter"],
        category: "An√°lise de Dados",
        subcategories: ["EDA", "Visualiza√ß√£o", "Estat√≠stica"],
        image: `${process.env.PUBLIC_URL}/projects/capa_spotify_analysis.png`,
        github: "https://github.com/tmarsbr/data-analyst-project",
        demo: "",
        metrics: "An√°lise de +50k m√∫sicas, identifica√ß√£o de 8 fatores-chave de sucesso",
        featured: true,
        complexity: 4,
        date: "2024"
    },
    {
        id: 2,
        title: "An√°lise dos Acidentes nas Rodovias Brasileiras",
        impactPhrase: "üéØ Projeto Social | An√°lise de Dados",
        description: "üõ£Ô∏è Analisei +100k registros de acidentes da PRF criando um mapa inteligente de seguran√ßa vi√°ria que identifica pontos cr√≠ticos em 27 estados brasileiros.",
        longDescription: "Este projeto nasceu de uma miss√£o pessoal: usar dados para salvar vidas nas estradas. Analisando registros da Pol√≠cia Rodovi√°ria Federal, criei visualiza√ß√µes interativas que revelam os pontos cr√≠ticos de acidentes em todo territ√≥rio nacional. O mais impactante foi descobrir padr√µes inesperados entre localiza√ß√£o de radares e redu√ß√£o de acidentes, gerando insights que podem influenciar pol√≠ticas p√∫blicas de seguran√ßa. Mapiei 27 estados e identifiquei os hor√°rios, condi√ß√µes clim√°ticas e trechos mais perigosos, criando um verdadeiro 'GPS da seguran√ßa' para as rodovias brasileiras.",
        technologies: ["Python", "Pandas", "Geopandas", "Plotly", "Folium"],
        category: "An√°lise de Dados",
        subcategories: ["EDA", "Visualiza√ß√£o", "Estat√≠stica"],
        image: `${process.env.PUBLIC_URL}/projects/capa_prf_accidents.png`,
        github: "https://github.com/tmarsbr/analise-PRF-",
        demo: "",
        metrics: "An√°lise de +100k acidentes, mapeamento de 27 estados",
        featured: true,
        complexity: 5,
        date: "2024"
    },
    {
        id: 3,
        title: "Pipeline de Integra√ß√£o - Cl√≠nicas Sanare e Reviver",
        impactPhrase: "‚ö° Projeto Real | Engenharia de Dados",
        description: "üè• Desenvolvi um pipeline ETL robusto que unificou sistemas de duas cl√≠nicas m√©dicas, migrando +10k registros com 99.9% de precis√£o e zero downtime.",
        longDescription: "Enfrentei um desafio real do mundo corporativo: duas cl√≠nicas m√©dicas se fundiram e precisavam unificar seus dados de pacientes, hist√≥ricos e procedimentos. O problema? Sistemas completamente diferentes, formatos incompat√≠veis e zero margem para erros - afinal, eram dados de sa√∫de humana. Desenvolvi uma solu√ß√£o elegante usando programa√ß√£o orientada a objetos, criando um pipeline ETL modular que n√£o apenas integrou os dados, mas tamb√©m implementou valida√ß√µes rigorosas de qualidade. O resultado? Uma migra√ß√£o 100% bem-sucedida que permitiu √† nova empresa operar desde o primeiro dia.",
        technologies: ["Python", "OOP", "ETL", "Data Quality", "Pandas"],
        category: "Engenharia de Dados",
        subcategories: ["ETL/ELT"],
        image: `${process.env.PUBLIC_URL}/projects/capa_integracao_sistemas_medicos.png`,
        github: "https://github.com/tmarsbr/projeto_pipeline",
        demo: "",
        metrics: "Integra√ß√£o de +10k registros, 99.9% de precis√£o na migra√ß√£o",
        featured: true,
        complexity: 5,
        date: "2024"
    },
    {
        id: 4,
        title: "Extra√ß√£o e An√°lise - Reposit√≥rios GitHub",
        impactPhrase: "üî• Automatiza√ß√£o | API & Scraping",
        description: "üêô Sistema automatizado que extraiu e analisou dados de +1000 reposit√≥rios de 15 grandes empresas tech via API GitHub, revelando tend√™ncias de desenvolvimento.",
        longDescription: "Sistema automatizado para coleta e an√°lise de dados de reposit√≥rios GitHub de grandes empresas de tecnologia. Utilizou API do GitHub para extrair informa√ß√µes sobre linguagens, atividade e tend√™ncias de desenvolvimento, gerando insights sobre o ecossistema tech.",
        technologies: ["Python", "GitHub API", "Pandas", "Requests", "JSON"],
        category: "API & Scraping",
        subcategories: ["Scraping", "API REST"],
        image: `${process.env.PUBLIC_URL}/projects/capa_github_analysis.png`,
        github: "https://github.com/tmarsbr/Projeto_api",
        demo: "",
        metrics: "An√°lise de +1000 reposit√≥rios, 15 empresas tech",
        featured: false,
        complexity: 3,
        date: "2024"
    },
    {
        id: 5,
        title: "Pipeline Python - MongoDB - MySQL",
        impactPhrase: "üöÄ Integra√ß√£o | Engenharia de Dados",
        description: "üîÑ Pipeline completo para e-commerce integrando MongoDB e MySQL, reduzindo em 70% o tempo de an√°lise da equipe de BI com processamento automatizado.",
        longDescription: "Desenvolvimento de pipeline completo para processamento de dados de e-commerce, integrando diferentes bases de dados. Solu√ß√£o automatizada para ETL entre MongoDB (dados n√£o-estruturados) e MySQL (dados estruturados), com foco em performance e confiabilidade.",
        technologies: ["Python", "MongoDB", "MySQL", "ETL", "PyMongo"],
        category: "Engenharia de Dados",
        subcategories: ["ETL/ELT", "SQL"],
        image: `${process.env.PUBLIC_URL}/projects/capa_pipeline_mongo_mysql.png`,
        github: "https://github.com/tmarsbr/pipeline-python-mongo-mysql",
        demo: "",
        metrics: "Redu√ß√£o de 70% no tempo de an√°lise da equipe de BI",
        featured: true,
        complexity: 4,
        date: "2024"
    },
    {
        id: 6,
        title: "An√°lise de Cr√©dito com Machine Learning",
        impactPhrase: "üõ†Ô∏è Em Desenvolvimento | Ci√™ncia de Dados",
        description: "üí≥ Modelo de score de cr√©dito com Machine Learning em fase de testes.",
        longDescription: "Projeto de an√°lise de cr√©dito utilizando t√©cnicas de machine learning para avalia√ß√£o de risco. Em desenvolvimento com foco em algoritmos de classifica√ß√£o e an√°lise de padr√µes de inadimpl√™ncia.",
        technologies: ["Python", "Scikit-learn", "Pandas", "Machine Learning"],
        category: "Ci√™ncia de Dados",
        hidden: true,
        subcategories: ["ML Cl√°ssico"],
        image: `${process.env.PUBLIC_URL}/projects/capa_credito_ml.png`,
        github: "",
        demo: "",
        metrics: "",
        featured: false,
        inDevelopment: true,
        date: "Em breve"
    },
    {
        id: 7,
        title: "People Analytics - Insights de RH",
        impactPhrase: "üöß Em Desenvolvimento | Ci√™ncia de Dados",
        description: "üë• Sistema de an√°lise de dados de RH para insights estrat√©gicos em gest√£o de pessoas.",
        longDescription: "Projeto focado na aplica√ß√£o de People Analytics para tomada de decis√£o em gest√£o de pessoas, incluindo an√°lise de turnover, performance e engajamento de colaboradores.",
        technologies: ["Python", "Pandas", "Plotly", "Statistics"],
        category: "Ci√™ncia de Dados",
        hidden: true,
        subcategories: ["ML Cl√°ssico"],
        image: `${process.env.PUBLIC_URL}/projects/capa_people_analytics.png`,
        github: "",
        demo: "",
        metrics: "",
        featured: false,
        inDevelopment: true,
        date: "Em breve"
    },
    {
        id: 8,
        title: "Previs√£o de Demandas - S√©ries Temporais",
        impactPhrase: "‚è±Ô∏è Em Constru√ß√£o | Ci√™ncia de Dados",
        description: "üìà Modelos de previs√£o de demanda utilizando algoritmos de s√©ries temporais.",
        longDescription: "Projeto focado em previs√£o de demandas utilizando algoritmos de s√©rie temporal avan√ßados, incluindo ARIMA, Prophet e redes neurais para forecasting empresarial.",
        technologies: ["Python", "Prophet", "ARIMA", "TensorFlow"],
        category: "Ci√™ncia de Dados",
        hidden: true,
        subcategories: ["ML Cl√°ssico"],
        image: `${process.env.PUBLIC_URL}/projects/capa_previsao_demandas.png`,
        github: "",
        demo: "",
        metrics: "",
        featured: false,
        inDevelopment: true,
        date: "Em breve"
    },
    {
        id: 9,
        title: "Sistema Antifraude com IA",
        impactPhrase: "üîí Em Desenvolvimento | Ci√™ncia de Dados",
        description: "üõ°Ô∏è Sistema de detec√ß√£o de fraudes com m√©todos estat√≠sticos e machine learning.",
        longDescription: "Modelo de escore antifraude utilizando t√©cnicas avan√ßadas de machine learning para detectar padr√µes suspeitos e prevenir fraudes em transa√ß√µes financeiras.",
        technologies: ["Python", "Scikit-learn", "Anomaly Detection", "Deep Learning"],
        category: "Ci√™ncia de Dados",
        hidden: true,
        subcategories: ["ML Cl√°ssico"],
        image: `${process.env.PUBLIC_URL}/projects/capa_fraude_financeira.png`,
        github: "",
        demo: "",
        metrics: "",
        featured: false,
        inDevelopment: true,
        date: "Em breve"
    },
    {
        id: 10,
        title: "Automatizando Infraestrutura de Processamento de Dados com AWS EMR e Apache Flink",
        impactPhrase: "‚òÅÔ∏è Cloud Infrastructure | Engenharia de Dados",
        description: "‚ö° Infraestrutura como c√≥digo para processamento de big data em tempo real utilizando AWS EMR, Apache Flink e Terraform para escalabilidade autom√°tica.",
        longDescription: "Projeto focado na automa√ß√£o completa de infraestrutura de processamento de dados em nuvem. Utilizando AWS EMR (Elastic MapReduce) para clusters gerenciados e Apache Flink para processamento de streams em tempo real, toda a infraestrutura √© provisionada via Terraform seguindo pr√°ticas de IaC (Infrastructure as Code). O sistema inclui auto-scaling, monitoramento integrado e otimiza√ß√£o de custos, demonstrando como construir pipelines de dados robustos e escal√°veis na AWS.",
        technologies: ["AWS EMR", "Apache Flink", "Terraform", "IaC", "Python"],
        category: "Engenharia de Dados",
        hidden: true,
        subcategories: ["Cloud AWS", "IaC"],
        image: `${process.env.PUBLIC_URL}/projects/capa_aws_emr_flink.png`,
        github: "https://github.com/tmarsbr/aws-emr-flink-portfolio",
        demo: "",
        metrics: "Infraestrutura 100% automatizada, processamento em tempo real",
        featured: true,
        complexity: 5,
        date: "2024"
    },
    {
        id: 11,
        title: "Pipeline PySpark Para Extrair, Transformar e Carregar Arquivos JSON em Banco de Dados",
        impactPhrase: "üî• Big Data Processing | Engenharia de Dados",
        description: "üöÄ Pipeline robusto de ETL desenvolvido com PySpark para processar grandes volumes de dados JSON, aplicando transforma√ß√µes complexas e carregamento otimizado com processamento distribu√≠do.",
        longDescription: "Imagine uma empresa que coleta dados de APIs, logs de aplica√ß√µes ou sensores IoT, todos em formato JSON. Esses dados precisam ser extra√≠dos, limpos, transformados e carregados em um banco de dados relacional ou NoSQL para an√°lises posteriores. Como engenheiro de dados, meu desafio era criar um pipeline escal√°vel que pudesse processar grandes volumes de JSONs, garantindo integridade, performance e facilidade de manuten√ß√£o.",
        technologies: ["PySpark", "Apache Spark", "JSON", "ETL", "SQL", "Processamento Distribu√≠do"],
        category: "Engenharia de Dados",
        hidden: true,
        subcategories: ["ETL/ELT", "PySpark", "DataOps", "Docker", "SQL/NoSQL"],
        image: `${process.env.PUBLIC_URL}/projects/capa_pyspark_etl_json.png`,
        github: "https://github.com/tmarsbr/pipeline-pyspark-etl-json",
        demo: "",
        metrics: "Processamento distribu√≠do, transforma√ß√µes complexas, escalabilidade horizontal",
        featured: true,
        complexity: 4,
        date: "2024"
    },
    {
        id: 12,
        title: "Pipeline de Limpeza e Transforma√ß√£o Para Aplica√ß√µes de IA com PySpark SQL",
        impactPhrase: "ü§ñ AI Data Preparation | Engenharia de Dados",
        description: "‚ú® Pipeline especializado em prepara√ß√£o de dados para modelos de IA usando PySpark SQL, garantindo qualidade e consist√™ncia dos datasets de treinamento.",
        longDescription: "Pipeline avan√ßado de prepara√ß√£o de dados especificamente desenhado para alimentar aplica√ß√µes de Intelig√™ncia Artificial. Utilizando PySpark SQL para opera√ß√µes eficientes, o sistema implementa t√©cnicas sofisticadas de limpeza, detec√ß√£o de anomalias, feature engineering e normaliza√ß√£o. Inclui valida√ß√µes automatizadas de qualidade, tratamento inteligente de valores ausentes e gera√ß√£o de estat√≠sticas descritivas para garantir que os dados estejam prontos para treinamento de modelos de ML.",
        technologies: ["PySpark", "Spark SQL", "Feature Engineering", "Data Quality"],
        category: "Engenharia de Dados",
        hidden: true,
        subcategories: ["ETL/ELT"],
        image: `${process.env.PUBLIC_URL}/projects/capa_pyspark_ai_pipeline.png`,
        github: "https://github.com/tmarsbr/pyspark-ai-data-pipeline",
        demo: "",
        metrics: "Prepara√ß√£o de dados para IA, valida√ß√µes automatizadas",
        featured: true,
        complexity: 5,
        date: "2024"
    },
    {
        id: 13,
        title: "Automa√ß√£o de Testes de Modelos de Dados no dbt",
        impactPhrase: "üß™ Data Testing | Engenharia de Dados",
        description: "üîç Framework completo de testes automatizados para modelos de dados usando dbt, garantindo qualidade e confiabilidade dos pipelines anal√≠ticos.",
        longDescription: "Implementa√ß√£o de um framework robusto de testes automatizados para modelos de dados utilizando dbt (data build tool). O sistema inclui testes de integridade referencial, valida√ß√µes de qualidade de dados, testes de regress√£o e monitoramento cont√≠nuo. Desenvolvido com foco em DataOps, o projeto demonstra como implementar CI/CD para dados, incluindo testes unit√°rios para transforma√ß√µes SQL, valida√ß√µes de schema e alertas autom√°ticos para anomalias nos dados.",
        technologies: ["dbt", "SQL", "Data Testing", "DataOps", "CI/CD"],
        category: "Engenharia de Dados",
        hidden: true,
        subcategories: ["DataOps"],
        image: `${process.env.PUBLIC_URL}/projects/capa_dbt_automated_testing.png`,
        github: "https://github.com/tmarsbr/dbt-automated-testing",
        demo: "",
        metrics: "Framework de testes automatizados, qualidade de dados garantida",
        featured: true,
        complexity: 4,
        date: "2024"
    },
    {
        id: 14,
        title: "Movimenta√ß√£o de Dados Entre Bancos de Dados com Airbyte",
        impactPhrase: "üîÑ Data Integration | Engenharia de Dados",
        description: "üåê Solu√ß√£o de integra√ß√£o de dados usando Airbyte para sincroniza√ß√£o autom√°tica entre diferentes fontes de dados, garantindo consist√™ncia e atualiza√ß√£o em tempo real.",
        longDescription: "Implementa√ß√£o de uma solu√ß√£o completa de integra√ß√£o de dados utilizando Airbyte para orquestrar a movimenta√ß√£o entre diferentes sistemas de banco de dados. O projeto demonstra como configurar conectores personalizados, implementar transforma√ß√µes durante a sincroniza√ß√£o e garantir a consist√™ncia dos dados entre ambientes. Inclui monitoramento de performance, tratamento de falhas e estrat√©gias de recupera√ß√£o, mostrando como construir pipelines de dados resilientes e escal√°veis.",
        technologies: ["Airbyte", "PostgreSQL", "MySQL", "Data Integration", "ETL"],
        category: "Engenharia de Dados",
        hidden: true,
        subcategories: ["ETL/ELT"],
        image: `${process.env.PUBLIC_URL}/projects/capa_automacao_etl.png`,
        github: "https://github.com/tmarsbr/airbyte-etl-portfolio",
        demo: "",
        metrics: "Sincroniza√ß√£o autom√°tica entre DBs, integra√ß√£o de dados resiliente",
        featured: true,
        complexity: 4,
        date: "2024"
    },
    {
        id: 15,
        title: "Pipeline de Dados Clim√°ticos ‚Äì Airflow",
        impactPhrase: "üå§Ô∏è Orquestra√ß√£o | Engenharia de Dados",
        description: "‚ö° Pipeline automatizado com Apache Airflow que extrai dados meteorol√≥gicos da API Visual Crossing Weather, processa e estrutura datasets semanalmente para planejamento tur√≠stico em Boston.",
        longDescription: "Desenvolvimento de um pipeline robusto de dados clim√°ticos utilizando Apache Airflow para uma empresa de turismo em Boston. O sistema resolve o desafio de coletar e processar dados meteorol√≥gicos de forma consistente e automatizada, permitindo planejamento inteligente de roteiros tur√≠sticos baseados em condi√ß√µes clim√°ticas. Implementa DAGs (Directed Acyclic Graphs) que extraem dados da API Visual Crossing Weather, processam informa√ß√µes meteorol√≥gicas e armazenam datasets organizados por semana. O pipeline inclui separa√ß√£o especializada de dados (temperaturas, condi√ß√µes clim√°ticas) e execu√ß√£o semanal automatizada, capacitando decis√µes data-driven que melhoram a experi√™ncia do cliente e otimizam opera√ß√µes tur√≠sticas.",
        technologies: ["Apache Airflow", "Python", "API Integration", "ETL", "pandas"],
        category: "Engenharia de Dados",
        subcategories: ["DataOps", "ETL/ELT"],
        image: `${process.env.PUBLIC_URL}/projects/capa_pipeline_climatico_airflow.png`,
        github: "https://github.com/tmarsbr/airflowalura",
        demo: "",
        metrics: "Pipeline semanal automatizado, extra√ß√£o de dados meteorol√≥gicos estruturados",
        featured: true,
        complexity: 4,
        date: "2024"
    },
    {
        id: 16,
        title: "Pipeline ETL Distribu√≠do com Apache Airflow e AWS EMR",
        impactPhrase: "üöÄ Enterprise-Ready | Big Data Engineering",
        description: "‚ö° Pipeline completo de ETL processando 5.8M registros de voos com Apache Airflow e AWS EMR, demonstrando arquitetura enterprise para processamento distribu√≠do em escala.",
        longDescription: "Projeto completo de Data Engineering de n√≠vel profissional que implementa um pipeline ETL robusto para processamento de big data. O sistema processa 5,819,079 registros de voos (564.96 MB) convertendo dados de CSV para formato Parquet otimizado, utilizando Apache Airflow 2.8.2 para orquestra√ß√£o e AWS EMR 6.15.0 com Spark 3.4.1 para processamento distribu√≠do. Inclui containeriza√ß√£o com Docker Compose, storage otimizado no S3 com particionamento por ano/m√™s/dia, e configura√ß√£o completa de VPC + IAM para seguran√ßa enterprise. O projeto demonstra resolu√ß√£o de desafios t√©cnicos reais incluindo compatibilidade de inst√¢ncias AWS (m5‚Üím4), configura√ß√£o VPC obrigat√≥ria, permiss√µes IAM corretas, capacidade de zona e corre√ß√£o de tipos de dados no Spark. Documenta√ß√£o completa para diferentes audi√™ncias (t√©cnica e executiva) e pr√°ticas de produ√ß√£o com monitoramento, logs detalhados e auto-termina√ß√£o para otimiza√ß√£o de custos.",
        technologies: ["Apache Airflow", "AWS EMR", "Apache Spark", "Docker", "S3", "Parquet", "VPC"],
        category: "Engenharia de Dados",
        hidden: true,
        subcategories: ["Cloud AWS", "DataOps", "ETL/ELT"],
        image: `${process.env.PUBLIC_URL}/projects/capa_airflow_emr_pipeline.png`,
        github: "https://github.com/tmarsbr/apache-airflow-emr-pipeline",
        demo: "",
        metrics: "5.8M registros processados, 564.96 MB otimizados, pipeline enterprise-ready",
        featured: true,
        complexity: 5,
        date: "2024"
    },
    {
        id: 17,
        title: "Constru√ß√£o de um Datalake e Lakehouse do Zero ‚Äì AWS & Databricks",
        impactPhrase: "üèóÔ∏è Data Lakehouse | Engenharia de Dados",
        description: "üèõÔ∏è Arquitetura moderna de dados implementando Datalake e Lakehouse do zero com AWS e Databricks, organizando dados em camadas RAW ‚Üí BRONZE ‚Üí SILVER ‚Üí GOLD.",
        longDescription: "Projeto completo de constru√ß√£o de uma arquitetura de dados moderna implementando conceitos de Datalake e Lakehouse utilizando AWS e Databricks. A solu√ß√£o organiza dados em camadas (RAW ‚Üí BRONZE ‚Üí SILVER ‚Üí GOLD) com CDC (Change Data Capture) e CDF (Change Data Feed) para ingest√£o e transforma√ß√£o cont√≠nua. Inclui processamento em tempo real, governan√ßa de dados, e cria√ß√£o de cubos anal√≠ticos na camada GOLD para alimentar dashboards e an√°lises de neg√≥cio. Demonstra dom√≠nio em arquiteturas modernas de dados com foco em escalabilidade, performance e governan√ßa.",
        technologies: ["AWS", "Databricks", "Delta Lake", "CDC", "CDF", "Spark", "Data Lakehouse"],
        category: "Engenharia de Dados",
        subcategories: ["Cloud AWS", "Databricks", "ETL/ELT", "Streaming", "Data Lakehouse", "DataOps", "Dashboard"],
        image: `${process.env.PUBLIC_URL}/projects/capa_neon_data_lakehouse.png`,
        github: "",
        demo: "",
        metrics: "Arquitetura Lakehouse completa, processamento em tempo real com CDC/CDF",
        featured: true,
        complexity: 5,
        date: "2024"
    },
    {
        id: 18,
        title: "Pipeline CDC - Ingest√£o Automatizada Kaggle ‚Üí AWS S3",
        impactPhrase: "üîÑ Change Data Capture | Engenharia de Dados",
        description: "üìä Sistema de ETL automatizado com CDC que detecta mudan√ßas em datasets do Kaggle, gerando arquivos Parquet otimizados para alimentar Data Lake na AWS S3.",
        longDescription: "Pipeline de ingest√£o incremental (Parte 1/2 de arquitetura completa de Data Lake) que automatiza a extra√ß√£o de dados do Kaggle com Change Data Capture (CDC). O sistema detecta e captura automaticamente tr√™s tipos de opera√ß√µes: INSERT (novos registros), UPDATE (altera√ß√µes em registros existentes) e DELETE (registros removidos), gerando arquivos Parquet com compress√£o Snappy e metadados CDC estruturados. Implementa compara√ß√£o inteligente de snapshots (anterior vs atual) para identificar mudan√ßas, evitando reprocessamento completo de datasets. Utiliza Python Schedule para orquestra√ß√£o de execu√ß√µes peri√≥dicas, com retry logic e exponential backoff para resili√™ncia. Os dados s√£o organizados no S3 em duas camadas: full-load (snapshot completo inicial) e cdc/ (arquivos incrementais com timestamp). Alcan√ßa 70% de redu√ß√£o no tamanho de armazenamento comparado a CSV tradicional, preparando dados otimizados para consumo downstream em arquiteturas Delta Lake e Lakehouse. Inclui logging estruturado, tratamento robusto de erros e suporte a m√∫ltiplas tabelas via configura√ß√£o JSON, demonstrando dom√≠nio em processamento incremental, otimiza√ß√£o de storage cloud-native e automa√ß√£o de pipelines ETL enterprise-grade.",
        technologies: ["Python", "Pandas", "AWS S3", "Parquet", "Kaggle API", "CDC", "boto3", "PyArrow"],
        category: "Engenharia de Dados",
        subcategories: ["ETL/ELT", "Cloud AWS", "DataOps"],
        image: `${process.env.PUBLIC_URL}/projects/capa_pipeline_cdc_kaggle.png`,
        github: "https://github.com/tmarsbr/cdc-kaggle",
        demo: "",
        metrics: "1.5GB/m√™s processados, 24 exec./dia, 3-5min tempo m√©dio, 70% economia storage, 99.9% uptime",
        featured: true,
        complexity: 4,
        date: "2024"
    }
];

/**
 * Configura√ß√£o da Se√ß√£o de Projetos
 * @description Textos e limites para a se√ß√£o de projetos na Home
 */
export const projectsConfig = {
    title: "Projetos em Destaque",
    description: "Uma vitrine com os projetos que mostram minha evolu√ß√£o pr√°tica em dados ‚Äî da coleta √† modelagem, com impacto real.",
    maxProjects: 3
};

/**
 * Configura√ß√£o da P√°gina de Projetos
 * @description Textos e storytelling da p√°gina de projetos
 */
export const projectsPageConfig = {
    title: "Data & Analytics",
    subtitle: "Arquiteturas Escal√°veis ‚Ä¢ Pipelines Robustos ‚Ä¢ Insights Reais",
    description: "Da ingest√£o bruta √† intelig√™ncia de neg√≥cio. Cada projeto demonstra minha capacidade de desenhar arquiteturas resilientes, automatizar fluxos complexos e entregar dados confi√°veis para tomada de decis√£o.",
    philosophy: "C√≥digo limpo, documenta√ß√£o clara e foco em resolver problemas reais. Minha stack √© apenas a ferramenta; o objetivo √© gerar valor atrav√©s dos dados."
};
